---
title: "Prediction of Default on Home Equity Loans"
subtitle: "Final Report"
author: "Philippe Lambot"
date: "February 18, 2020"
output: 
  html_document:
    toc: true
    toc_depth: 4
---

```{r Setup, include = FALSE}

# In the YAML, I have asked a TOC (table of contents). 
# I have also chosen to produce an html_document and issue it in PDF format. 

# In the opts_chunk just below, I have chosen options to avoid messages and warnings in hmeq_Final_Report.html. Messages and warnings produced by the code have already been dealt with.  
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)

# The next opts_chunk regulates figure layout.
knitr::opts_chunk$set(out.width = "60%", fig.align = "center")

# The next instruction facilitates table layout in HTML.
options(knitr.table.format = "html")

# After the present chunk, there are 13 lines of code to further regulate layout:
# - the 1st block prevents bullets appearing in the TOC (Table of Contents);
# - the 2nd block determines font size in body text parts;
# - the 3rd block generates text justification.

# Last about layout, I use the string <br> to generate empty lines.
```

<style type="text/css">
div#TOC li {
    list-style:none;
    background-image:none;
    background-repeat:none;
    background-position:0;
}
</style>

<font size="3">

<style>
body {
text-align: justify}
</style>

## <br>
## <br>
## <br>

## *********************************************************************************

## Prediction of Default on Home Equity Loans

## ********************************************************************************* 

<br>

## Executive Summary

<br>

Welcome to this data science project dedicated to default prediction on home equity loans. Data have been downloaded from the Kaggle website: https://www.kaggle.com/ajay1735/hmeq-data . 
Two avenues of modeling have been opened up due to predictor information being missing in approximately 2,500 observations out of 6,000, as evidenced in data profiling. 

On the first hand, the subset with complete information has been dealt with in machine learning in a multi-tier process: pre-testing 21 algorithms such as eXtreme Gradient Boosting, Stochastic Gradient Boosting or Monotone Multi-Layer Perceptron Neural Network; preselecting 6 of them according to performance metrics for the standard 0.5 probability threshold; fine_tuning probability threshold on a broad range of thresholds and selecting 3 algorithms, i.e. AdaBoost Classification Trees and 2 Random Forests algorithms; combining them into an ensemble model by majority vote and eventually on the basis of the average probabilities. 

On the other hand, the subset with incomplete predictor information has been dealt with separately in machine learning by utilizing the relation between missing information and default rate, which had been evidenced in the exploratory data analysis.   

The dual validation process delivered global performance metrics of 84 % sensitivity and 61 % precision on the global validation set, which largely exceeded the global target of 75-50. 

The whole machine learning model could be rather easily parameterized, in e.g. a dashboard, to aim at different trade-offs between sensitivity and precision. We leave this promising avenue of research to the future. 

<br>

TAGS: machine learning, classification, missing values, eXtreme Gradient Boosting, AdaBoost Classification Trees, Random Forests, probability threshold, ensemble models, recall-precision curves, R; home equity loans, finance, banking

<br>

GitHub: https://github.com/Dev-P-L/Home-Equity-Loan-Default-Prediction

<br>

## I. Business Case: why, what, how, who and where?

### A. Why?

#### 1. Why that topic?

<br>

**From a domain point of view**, predicting default on home equity loans is an important issue for lenders but probably even much more for borrowers. 

First, **lenders** are at risk e.g. if they offer an amount worth more than 100% of the equity; and indeed some lenders offer home equity loans of an amount worth up to 125% of the equity (https://www.investopedia.com/personal-finance/home-equity-loans-what-to-know/ ). They are also at risk e.g. if there is a general decrease in real estate prices and in that case defaults are no longer statistically independent since multiple borrowers can be hit by the same factor.

Second, **borrowers** are also at risk, especially if they do some "reloading", which has been clearly defined by Investopedia: "The main pitfall associated with home-equity loans is that they sometimes seem to be an easy solution for a borrower who may have fallen into a perpetual cycle of spending, borrowing, spending, and sinking deeper into debt. Unfortunately, this scenario is so common that lenders have a term for it: reloading, which is basically the habit of taking a loan in order to pay off existing debt and free up additional credit, which the borrower then uses to make additional purchases."  (https://www.investopedia.com/personal-finance/home-equity-loans-what-to-know/, retrieved on January 22, 2020). 

Third, it is also a sensitive matter of **economic, financial and social policy**, just as it could be seen not so long ago. 

In a snapshot, it is an important business and societal issue. 

**From a machine learning point of view**, it is a classification challenge on structured data, i.e. an opportunity to use some algorithms that are often dithyrambically praised for classification challenges on structured data, such as eXtreme Gradient Boosting, Stochastic Gradient Boosting and Random Forests. Please see e.g.:

*XGBoost is a decision-tree-based ensemble Machine Learning algorithm that uses a      gradient boosting framework. In prediction problems involving unstructured data (images, text, etc.) artificial neural networks tend to outperform all other algorithms or frameworks. However, when it comes to small-to-medium structured/tabular data, decision tree based algorithms are considered best-in-class right now.* 

*Since its introduction, this algorithm has not only been credited with winning numerous Kaggle competitions but also for being the driving force under the hood for several cutting-edge industry applications.*
(https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d, retrieved on January 22, 2020)

<br>

#### 2. Why this dataset?

<br>

**From a domain point of view**, this dataset has gotten a flavor of professionalism: indeed, these predictors are relevant from a professional standpoint, even if the very list of predictors seems far from complete and even if predictor content is just succinctly explicated. 

**From a data science point of view**, this dataset represents a big challenge in terms of  data profiling and data preparation since there are numerous missing values as can be seen when scrolling on the Kaggle website. This is an interesting feature since it is a trait common to many real-life data science projects, where data preparation is often time-consuming: 

*[...] according to a 2018 IDC Business Analytic Solutions survey, data scientists spend 73% of their time doing the hard work of preparing data for more, value-added activities like predictive analytics or forecasting.* (https://www.import.io/post/what-is-data-cleansing-and-transformation-wrangling/, retrieved on January 22, 2020). 

<br>

### B. What?

<br>

What will this project be?

It is an essentially **predictive project** in which A-predictor algorithms will be used to optimize performance metrics. 

There is also a **descriptive aspect**, especially in data profiling and exploratory data analysis since both dimensions are often essential to efficient prediction. In exploratory data analysis, some essential information will be retrieved about frequencies of default according to category, especially frequencies of default when information is missing. 

Third, there might be a somewhat **presciptive message** as well although this is not the main objective.

There is **no explicative approach** as could be the case with logistic or linear regressions where the influence of each predictor would be quantified and its statistical significance would be tested. The main aim is a predictive one.

Since **domain knowledge** is an essential part of the job in real-life projects, some domain considerations also be expressed, marginally though.  

<br>

### C. How?

#### 1. How Will Prediction Performance Be Evaluated? 

<br>

As stated on the Kaggle website, defaults are a minority case among loans, more specifically 20%. In other words, **prevalence of defaults is limited**.  

In such a case, general accuracy of prediction (percentage of correctly predicted outcomes) is not a relevant performance metric: indeed, with a limited percentage of defaults, predicting only non-defaults can already deliver a rather high accuracy while actually predicting absolutely no default! 

Consequently, let's turn to a combination of two other performance metrics: sensitivity or recall, i.e. the percentage of correctly predicted defaults with respect to the total number of defaults, and precision or positive predictive value, i.e. the percentage of correctly predicted defaults with respect to the total number of predicted defaults. 

Setting targets in advance for sensitivity and precision is pretty hard for at least two reasons. First, data have not been analyzed yet. Second, there is no loan manager to tell us what trade-off he wishes between sensitivity and precision: is he risk-averse or is he rather tempted to minimize the number of rejected loan requests? It will be assumed that  he is rather risk-averse: priority will be given to sensitivity and **target will be fixed at 75 % sensitivity and 50 % precision**. 

<br>

#### 2. How Will Data Be Prepared and Treated? 

<br>

In light of the numerous missing values, some solid data profiling will be required. After data profiling, which will be explained in the next section, specific action will be taken and described in the same section. 

After data profiling and data preparation, a full exploratory data analysis will be conducted on the training set, in the form of graphs and tables. Much time will be allocated to exploratory data analysis since it often informs the data science and machine learning processes in a pivotal way.   

<br>

### D. Who?

<br>

In this project, I am excited about using my dual professional experience in finance and data treatment and the outstanding hands-on experience provided by data science MOOCs delivered on the edX platform by Harvard and the MIT (https://credentials.edx.org/records/programs/shared/c385680c1ac342d7bc16800f9ee57b44/, https://courses.edx.org/certificates/9dc9f30a2e47414e9270ceadbda172d2 ). 

Let me also thank my friend Richard Careaga, former Associate General Counsel, JPMorgan Chase, N.A., for indefatigable questioning and discussion that were both valuable and invaluable from a domain point of view and from a data science standpoint. Interaction was most inspiring and supportive. 

<br> 

### E. Where?

<br>

This project is lodged with the GitHub repository https://github.com/Dev-P-L/Home-Equity-Loan-Default-Prediction.

It is comprised of five files:

- hmeq.csv,
- Attribute_Description.csv,
- hmeq_Script.Rmd,
- hmeq_Final_Report.html
- and README.md

All code is included in hmeq_Script.Rmd. It does not show in hmeq_Final_Report.html. 

Dear Readers, you are most welcome to run the project on your own computer if you so wish. 
First, for your convenience, the dataset has already been downloaded onto my GitHub repository 
https://github.com/Dev-P-L/Home-Equity-Loan-Default-Prediction wherefrom it will be automatically retrieved by hmeq_Script.Rmd code. If you so wish, you can also
easily retrieve the dataset from https://www.kaggle.com/ajay1735/hmeq-data and adapt code from hmeq_Script.Rmd.

Second, you can knit hmeq_Script.Rmd (please in HTML) and produce hmeq_Final_Report.html on your own computer. 

On my laptop, running hmeq_Script.Rmd takes approximately one and a half hour.  

For information, here are some characteristics of my environment:

 - R version 3.5.1 (2018-07-02) -- "Feather Spray",
 - RStudio Version 1.1.456 - © 2009-2018,
 - Windows 10.

Some packages are required by hmeq_Script.Rmd. The code from hmeq_Script.Rmd contains instructions to download these packages if they are not available yet.

<br>

```{r Cleaning up workspace and downloading packages}

# Cleaning up workspace for RAM management.
invisible(if(!is.null(dev.list())) dev.off())
rm(list=ls())
cat("\014")

# Downloading packages.
if(!require(tidyverse)) install.packages("tidyverse", repos = "http://cran.us.r-project.org")
if(!require(kableExtra)) install.packages("kableExtra", repos = "http://cran.us.r-project.org")
if(!require(caret)) install.packages("caret", repos = "http://cran.us.r-project.org")
if(!require(randomForest)) install.packages("randomForest", repos = "http://cran.us.r-project.org")
if(!require(xgboost)) install.packages("xgboost", repos = "http://cran.us.r-project.org")
if(!require(wsrf)) install.packages("wsrf", repos = "http://cran.us.r-project.org")
if(!require(gam)) install.packages("gam", repos = "http://cran.us.r-project.org")
if(!require(mgcv)) install.packages("mgcv", repos = "http://cran.us.r-project.org")
if(!require(fastAdaboost)) install.packages("fastAdaboost", repos = "http://cran.us.r-project.org")
```

## II. Data Source Investigation

<br>

On the Kaggle site, there is a description of the dataset. It is a rather summary description. 

Information about the data source is limited: 

*The Home Equity dataset (HMEQ) contains baseline and loan performance information for 5,960 recent home equity loans. The target (BAD) is a binary variable indicating whether an applicant eventually defaulted or was seriously delinquent. This adverse outcome occurred in 1,189 cases (20%).*

This seems to relate to real and recent data about granted home equity loans. How recent? Since the dataset was updated 2 years ago, this might be an indication that data are really recent. 

There is also a reference to the Equal Credit Opportunity Act, which is a "law (codified at 15 U.S.C. § 1691 et seq.), enacted 28 October 1974" (https://en.wikipedia.org/wiki/Equal_Credit_Opportunity_Act, retrieved on February 8, 2020). Consequently, it could be assumed that this dataset relates to the United States. 

<br>

## III. Data Profiling, Insights and Data Preparation

### A. Having a Look at the Target Variable and the Predictors

<br>

```{r Downloading data}
# Data has been downloaded from the Kaggle site: https://www.kaggle.com/ajay1735/hmeq-data # and have been uploaded onto GitHub under the name "hmeq.csv". 

# Now, let's retrieve hmeq.csv from my GitHub repository by accessing #https://raw.githubusercontent.com/Dev-P-L/Home-Equity-Loan-Default-Prediction/master/hmeq# .csv and by using the read.csv() function.

myfile <- "https://raw.githubusercontent.com/Dev-P-L/Home-Equity-Loan-Default-Prediction/master/hmeq.csv"
hmeq <- read.csv(myfile)
rm(myfile)
```

Before profiling data, attribute names will be somewhat reworded, just for clarity. So, we can immediately start with the definitive terminology and we do not have to change it later on. The same will be done for the values taken by the target variable: "Default" and "Repaid" will be used in the target variable instead of 1 and 0.  The next table gives the variable types and a brief description of the attributes.

<br>

```{r Data Preparation}

# Importing Attribute_Description.csv with already adapted attribute names.
myfile <- "https://raw.githubusercontent.com/Dev-P-L/Home-Equity-Loan-Default-Prediction/master/Attribute_Description.csv"
ad <- read.csv(myfile)
rm(myfile)

# Changing the attribute names in the data frame hmeq.
colnames(hmeq) <- ad$New_Name

# Changing the symbols from the target variable.
hmeq$y <- gsub(1, "Default", hmeq$y)
hmeq$y <- gsub(0, "Repaid", hmeq$y)
hmeq <- hmeq %>% mutate(y = as.factor(y)) %>% as.data.frame()

# Printing attribute table.
kable(ad, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
  full_width = F, font_size = 16)
rm(ad)
```

<br>

### B. Data Profiling

<br>

Let's have a look at the first rows from the data frame hmeq. 

<br>

```{r Showcasing the first rows from the data frame hmeq, slightly reworded}
df <- head(hmeq)
kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
  full_width = F, font_size = 16)
rm(df)
```

<br>

A few statements can be made readily, and among them:

- there are numerous missing values;
- in the factors, i.e. the attributes "REASON" and "JOB", missing values are not marked as "NA".

Tackling the second challenge is no problem: it suffices to replace white spaces with NAs to indicate that these values are actually missing. This being done, let's calculate the number of missing values and the number of rows with at least one missing value and compare with totals in the data frame hmeq. 

<br> 

```{r Replacing white spaces with NAs and calculating statistics about missing values}

# Marking missing values that do not appear as such as "NA".
hmeq <- hmeq %>% mutate(reason = as.character(reason), job = as.character(job))
hmeq$reason[hmeq$reason == ""] <- NA
hmeq$job[hmeq$job == ""] <- NA
hmeq <- hmeq %>% mutate(reason = as.factor(reason), job = as.factor(job))

# Numbers and percentages of values and missing values 
nr_mis <- sum(is.na(hmeq))
nr_values <- sum(!is.na(hmeq))

total_nr_positions <- nr_mis + nr_values
perc_mis <- nr_mis / total_nr_positions
perc_values <- nr_values / total_nr_positions

# Check on count
check <- total_nr_positions - (nrow(hmeq) * ncol(hmeq))
if(check != 0) {
  print("Problem related to the count of missing and non missing values!!!!")
}

# Numbers and percentages of rows with or without missing value
hmeq_mis <- hmeq[rowSums(is.na(hmeq)) > 0,]
nr_rows_mis <- nrow(hmeq_mis)
nr_rows_hmeq <- nrow(hmeq)
perc_rows_mis <- nr_rows_mis / nr_rows_hmeq

hmeq_clean <- hmeq[rowSums(is.na(hmeq)) == 0,] 
nr_rows_clean <- nrow(hmeq_clean)
perc_rows_clean <- nr_rows_clean / nr_rows_hmeq

# Checking count.
check <- nr_rows_mis + nr_rows_clean - nr_rows_hmeq
if(check != 0) {
  print("Problem related to the count of missing and non missing values!!!!")
}

# Table comprised of statistics about missing and non missing values
df_nr <- data.frame(v1 = format(nr_mis, big.mark = " "),
                    v2 = format(nr_values, big.mark = " "),
                    v3 = format(nr_rows_mis, big.mark = " "),
                    v4 = format(nr_rows_clean, big.mark = " "))

# Assembling statistics about PERCENTAGES of missing and non missing values.
perc_mis <- round(perc_mis * 100)
perc_values <- round(perc_values * 100)
perc_rows_mis <- round(perc_rows_mis * 100)
perc_rows_clean <- round(perc_rows_clean * 100)

# Table about percentages
df_per <- data.frame(v1 = paste(perc_mis, "%", sep = " "),
  v2 = paste(perc_values, "%", sep = " "),
  v3 = paste(perc_rows_mis, "%", sep = " "), 
  v4 = paste(perc_rows_clean, "%", sep = " "))

# Combining both tables.
df <- rbind(df_nr, df_per) 
df <- df %>% rename(Missing_Values = v1) %>% rename(Values = v2) %>%
  rename(Rows_with_Missing_Values = v3) %>% rename(Rows_without_Missing_Values = v4)

# Printing table.
kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 16)
 
rm(nr_mis, nr_values, total_nr_positions, check, perc_mis, perc_values, nr_rows_mis, nr_rows_hmeq, perc_rows_mis, nr_rows_clean, perc_rows_clean, df_nr, df_per, df)
```

<br>

The above table confirms that many values are missing: 5,271, i.e. 7% of the whole number of positions in the data frame hmeq. In terms of rows contaminated, it is even worse: 2,596 rows "contain"" at least one missing value, i.e. 44% of rows! 

These counts and percentages are rather impressive. To better evaluate the possible impact on predictiveness, let's localize the missing values per attribute. Indeed, some predictors might be less pivotal than others with respect to predictiveness and consequently missing values in such fields might be less damaging in terms of predictiveness.

The next table shows the number and the percentage of missing values per attribute.

<br>

```{r Table numbering missing values per attribute}

# First, numbering missing values per attribute.
list_colnames <- colnames(hmeq)
length_list <- length(list_colnames)
result <- 1:(length_list)
for (i in 1:(length_list)) {
  vector <- hmeq[, i]
  vector <- vector[is.na(vector) > 0]
  result[i] <- length(vector)
}

# Second, percentage of missing values per attribute
result_per <- round(result * 100 / nrow(hmeq))

# Third, table with numbers and percentages of missing values per attribute
df_nr <- data.frame(Attribute = list_colnames, 
  Number_of_Missing_Values = format(result, big.mark = " "),
  Percentage_of_Missing_Values = 
    paste(result_per, "%", sep = " "))

# Printing table.
kable(df_nr, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 16)

rm(list_colnames, length_list, result, i, vector, result_per, df_nr)
```

<br>

In data about debt-to-income ratio, 21 % values are missing. When such information is missing, it should be considered as a red flag. 

The same holds for the mortgage balance due; it seems essential in granting a home equity loan and yet 9 % values are missing!

The same again for several other predictors such as property value (2 % missing), the number of major derogatory reports (12 % missing), the number of delinquent lines (10 % missing), the number of recent credit lines (9 % missing) and the number of credit lines (4 % missing). 

These missing values should all be considered as red flags since these predictors are valuable and decisive information. A link between default rate and missing information would be hardly surprising! To check the assumption of such a link, the percentages of defaults on loans without missing information and on loans with missing information have been calculated. The next table shows a clear-cut difference in percentages of defaults. 

<br>

```{r Link between missing values and defaults}

# Default rates with and without missing value
default_rate_global <- sum(hmeq$y == "Default") / nrow(hmeq)
default_rate_without_missing_values <- 
  sum(hmeq_clean$y == "Default") / nrow(hmeq_clean)
default_rate_with_missing_values <- 
  sum(hmeq_mis$y == "Default") / nrow(hmeq_mis)

# Data frame with default rates
df <- data.frame(
  Default_on_All_Loans = 
    paste(round(default_rate_global * 100), "%", sep = " "),
  Default_when_Information_Complete = 
    paste(round(default_rate_without_missing_values * 100), "%", sep = " "),
  Default_when_Missing_Information = 
    paste(round(default_rate_with_missing_values * 100), "%", sep = " "))

# Printing table.
kable(df, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 16)

rm(default_rate_global, default_rate_without_missing_values, default_rate_with_missing_values, df)
```

<br>

### C. Insights

<br>

As it appears in the table above, the global default rate is 20 %, but it is only 9 % on loans with complete information and it jumps to 34 % on loans with missing information. Consequently, missing information is a rather powerful predictor of default! 

It is possible that missing information in some predictors might be a better indicator of default than missing information in other predictors: this will be investigated in the exploratory data analysis.

Should imputation be used? In particular, should an imputation method based on medians or averages be used? First, missing values seem to have some predictive power of default, which might disappear if missing values were replaced by imputed values. Second, with 5,271 missing values and 2,956 rows permeated, imputation seems risky, even temerarious, especially so in finance! 

Consequently, a completely different avenue of research will be investigated: splitting the dataset into a subset with all rows containing full information and a second subset with all rows containing at least one missing value. 

<br> 

### D. Data Preparation

<br>

So, there is a subset with complete information, hmeq_clean, and a subset with missing information, hmeq_mis. Both subsets will be split into a training set, a test set and a validation set.  Splitting into training set, test set and validation set will be done in an egalitarian way, each receiving one third, in order to keep statistical representativeness in spite of the rather limited size of the original dataset. 

<br>

```{r Splitting hmeq_clean and hmeq_mis each into a training set, a test set and a validation set}

# Splitting the dataset hmeq_clean (with complete information) into training set, test set and validation set.
set.seed(1)
ind <- createDataPartition(y = hmeq_clean$y, times = 1, p = 1/3, list = FALSE)
hmeq_clean_val <- hmeq_clean[ind,]
temp <- hmeq_clean[-ind,]

set.seed(1)
ind <- createDataPartition(y = temp$y, times = 1, p = 0.5, list = FALSE)
hmeq_clean_test <- temp[ind,]
hmeq_clean_train <- temp[-ind,]

# Splitting the dataset hmeq_mis (with incomplete information) into training set, test set and validation set.
set.seed(1)
ind <- createDataPartition(y = hmeq_mis$y, times = 1, p = 1/3, list = FALSE)
hmeq_mis_val <- hmeq_mis[ind,]
temp <- hmeq_mis[-ind,]

set.seed(1)
ind <- createDataPartition(y = temp$y, times = 1, p = 0.5, list = FALSE)
hmeq_mis_test <- temp[ind,]
hmeq_mis_train <- temp[-ind,]

rm(ind, temp)
```

After profiling data, getting insights and preparing data, let's move to exploratory data analysis.

<br>

## IV. Exploratory Data Analysis (EDA) and Insights

<br>

EDA will be performed on the combination of hmeq_clean_train and hmeq_mis_train, i.e. on the combination of the training set for loans with complete information and the training set for loans with incomplete information. From a communicational point of view, presenting 12 predictors only once looks like additional value. From an analytical point of view, per predictor, the average default rate in case of missing information will be the same as if calculated only on hmeq_mis_train. 

By the way, as far as the label (outcome) is concerned, default percentages are already known: 20 % globally, 9 % in hmeq_clean and 34 % in hmeq_mis. Moreover, we already know, from section *III. Data Profiling, Insights and Data Preparation*, that there are no missing values in the target variable. Consequently, we do not have to further analyze the label and we can immediately turn to the predictors. 

<br>

### A. EDA per Predictor

#### 1. Loan Amount

<br>

Let's split the default rate according to quartiles of loan amount. We already know, from section *III. Data Profiling, Insights and Data Preparation*, that there are no missing values in loan amounts. 

<br>

```{r Graph of default rate per quartile of loan amount}

# Combining hmeq_clean_train and hmeq_mis_train. 
# rbind() is used instead of bind_rows() because rbind() requires the same number of columns and checks whether it is effective or not.
hmeq_train <- rbind(hmeq_clean_train, hmeq_mis_train)

# Data frame with selected columns.
temp <- hmeq_train %>% select(y, loan) %>% mutate(y = as.numeric(y) - 1) 

# Calculating loan quartiles.
s <- summary(temp$loan)
df <- data.frame(intervals = as.character(names(s)), values = matrix(s), 
                 stringsAsFactors = FALSE)

# Removing the mean from the data frame to only keep quartiles, minimum and maximum.
row.to.remove <- which(df$intervals %in% "Mean")
df <- df[- row.to.remove,] 

# Enlarging range to include minimum and maximum.
v = df$values
v[1] <- floor(v[1])
v[length(v)] <- ceiling(tail(v, 1))

# Rounding for clarity of graph and table. 
for (i in 1:(length(v) - 2))  {
  v[i + 1] <- round(v[i + 1])
}

# Building up data frame with percentages of default per loan quartile.
df <- df %>% mutate(values = v)
tab <- temp %>% mutate(intervals = cut(loan, breaks = df$values, 
                       include.lowest = TRUE, right = TRUE, dig.lab = 5)) %>%
  group_by(intervals) %>% 
  summarize(n = n(), perc_def = round((1 - (sum(y) / n)) * 100)) %>% 
  select(intervals, n, perc_def) %>% as.data.frame()

# Drawing a geom_bar graph with default percentages. 
graph <-  tab %>%
  ggplot(aes(intervals, perc_def)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Default Rate per Quartile of Loan Amount") +
  xlab("Quartile of Loan Amount") + ylab("Default Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
```

<br>

From a methodological point of view, some rounding of quartiles has been performed for clarity of graph and table. If this affected quartile distribution in a significant way, this would show in the table below.

There is a big difference in average default rate between the first and the third interval. The next table expresses information in numbers. 

<br>

```{r Table of default rate per quartile of loan amount}
tab <- tab %>% mutate(perc_def = paste(perc_def, "%", sep = " ")) %>%
  rename(Loan_Amount = intervals, Loan_Number = n, Default_Rate = perc_def)
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 14) %>% column_spec(1:3, width = "2in")
rm(temp, s, df, row.to.remove, v, graph, tab)
```

<br>

#### 2. Mortgage Balance Due 

<br>

The mortgage balance due is the amount still due on the main mortgage (or first mortgage or preexisting mortgage; please see e.g. https://www.investopedia.com/terms/h/homeequityloan.asp ). It is an essential piece of information since the difference between the property current market value and the mortgage balance due determines the collateral left to back the home equity loan request. 

Let's have a look at a quartile-based graph. 

<br>

```{r Graph of default rate per quartile of mortgage balance due}

# Preparing data frames.
temp <- hmeq_train %>% select(y, mort_due) %>% mutate(y = as.numeric(y) - 1) 
temp_clean <- temp[is.na(temp$mort_due) == 0,] 
temp_na <- temp[is.na(temp$mort_due) == 1,]

# Calculating mort_due quartiles.
s <- summary(temp_clean$mort_due)
df <- data.frame(intervals = as.character(names(s)), values = matrix(s), 
                 stringsAsFactors = FALSE)

# Removing the mean from the data frame to only keep quartiles, minimum and maximum.
row.to.remove <- which(df$intervals %in% "Mean")
df <- df[- row.to.remove,] 

# Enlarging range to include minimum and maximum.
v = df$values
v[1] <- floor(v[1])
v[length(v)] <- ceiling(tail(v, 1))

# Rounding to upgrade clarity in graph and table.
for (i in 1:(length(v) - 2))  {
  v[i + 1] <- round(v[i + 1])
}

# Building up data frame with default percentages per mort_due quartile.
df <- df %>% mutate(values = v)
tab <- temp_clean %>% select(y, mort_due) %>% 
  mutate(intervals = cut(mort_due, breaks = df$values, 
                        include.lowest = TRUE, right = TRUE, dig.lab = 5)) %>%
  group_by(intervals) %>% 
  summarize(n = n(), perc_def = round((1 - (sum(y) / n)) * 100)) %>%  
  select(intervals, n, perc_def) %>% as.data.frame()

# Including default percentage in case of missing values in mort_due.
tab_2 <- data.frame(intervals = "NAs")
tab_2 <- tab_2 %>% 
  mutate(n = nrow(temp_na), perc_def =  round((1 - (sum(temp_na$y) / n)) * 100))

# Assembling tab and tab_2.
tab_3 <- rbind(tab, tab_2)

# Drawing a geom_bar graph with default percentages. 
graph <-  tab_3 %>%
  ggplot(aes(intervals, perc_def)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Default Rate per Quartile of Mortgage Balance Due") +
  xlab("Quartile of Mortgage Balance Due") + ylab("Default Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
```

<br>

There is a substantial difference in default rate between the first and the fourth interval. The next table expresses information in numbers. 

<br>

```{r Table of default rate per quartile of mortgage balance due}
tab <- tab_3 %>% mutate(
  perc_def = paste(perc_def, "%", sep = " ")) %>%
  rename(Mortgage_Balance_Due = intervals, Loan_Number = n, Default_Rate = perc_def)
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 14) %>% column_spec(1:3, width = "2in")
rm(temp, s, df, row.to.remove, v, graph, tab, tab_2, tab_3)
```

<br>

#### 3. Property Current Market Value

<br>

The property current market value is the third predictor. Let's have a look at the quartile-based graph. 

<br>

```{r Quartile-based graph for property current market value}

# Preparing data frames.
temp <- hmeq_train %>% select(y, prop_val) %>% mutate(y = as.numeric(y) - 1) 
temp_clean <- temp[is.na(temp$prop_val) == 0,] 
temp_na <- temp[is.na(temp$prop_val) == 1,]

# Calculating quartiles of property current market value.
s <- summary(temp_clean$prop_val)
df <- data.frame(intervals = as.character(names(s)), values = matrix(s), 
                 stringsAsFactors = FALSE)

# Removing the mean from the data frame to only keep quartiles, minimum and maximum.
row.to.remove <- which(df$intervals %in% "Mean")
df <- df[- row.to.remove,] 

# Enlarging range to encompass both minimum and maximum.
v = df$values
v[1] <- floor(v[1])
v[length(v)] <- ceiling(tail(v, 1))

# Rounding to upgrade clarity in graph and table.
for (i in 1:(length(v) - 2))  {
  v[i + 1] <- round(v[i + 1])
}

# Building up quartile-based data frame.
df <- df %>% mutate(values = v)
tab <- temp_clean %>% select(y, prop_val) %>% 
  mutate(intervals = cut(prop_val, breaks = df$values, 
                        include.lowest = TRUE, right = TRUE, dig.lab = 5)) %>%
  group_by(intervals) %>% 
  summarize(n = n(), perc_def = round((1 - (sum(y) / n)) * 100)) %>%  
  select(intervals, n, perc_def) %>% as.data.frame()

# Including default percentage in case of missing values in prop_val.
tab_2 <- data.frame(intervals = "NAs")
tab_2 <- tab_2 %>% 
  mutate(n = nrow(temp_na), perc_def =  round((1 - (sum(temp_na$y) / n)) * 100))

# Assembling tab and tab_2
tab_3 <- rbind(tab, tab_2)

# Drawing a geom_bar graph with default percentages.
graph <-  tab_3 %>%
  ggplot(aes(intervals, perc_def)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Default Rate per Property Value Quartile") +
  xlab("Quartile of Property Current Market Value") + ylab("Default Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
```

<br>

There is some difference in default rate especially between the first and the third interval. But the most striking statement is, by far, the impressive default rate when information is missing about property current market value. The next table expresses information in numbers. 

<br>

```{r Quartile-based table about property current market value}
tab <- tab_3 %>% mutate(
  perc_def = paste(perc_def, "%", sep = " ")) %>%
  rename(Property_Value = intervals, Loan_Number = n, Default_Rate = perc_def)
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 14) %>% column_spec(1:3, width = "2in")
rm(temp, s, df, row.to.remove, v, graph, tab, tab_2, tab_3)
```

<br>

The average default rate is at 25% in the first interval and at 14 % in the third interval. 

But much more striking: among the 45 loans without any information about property value, the default rate jumps to 96 %!

This predictor seems promising to discriminate between quartiles, but it is even more promising for missing values: the absence of property value information is a clear indicator of default! Of course, the percentage of 96 is calculated on only 45 loans: from a statistical point of view, standard error can be rather important. Nevertheless, it seems worthwhile to use this piece of information to predict default in case of missing information on the property current market value!

<br>

#### 4. Composite Predictor: Collateral

<br>

Lets' evaluate the predictive power of the collateral amount. Actually, the collateral amount is not included in the dataset hmeq. But it can be built up as the difference between, on the one hand, property current market value and, on the other hand, the total of mortgage balance due and loan amount. Then, default rate can be computed per collateral quartile.

<br>

```{r Quartile-based graph about composite predictor: collateral}

# Calculating collateral.
temp <- hmeq_train %>% mutate(collateral = (prop_val - mort_due - loan)) %>% 
  mutate(y = as.numeric(y) - 1) %>% select(y, collateral)

# Preparing data frames.
temp_clean <- temp[is.na(temp$collateral) == 0,] 
temp_na <- temp[is.na(temp$collateral) == 1,]

# Calculating collateral quartiles.
s <- summary(temp_clean$collateral)
df <- data.frame(intervals = as.character(names(s)), values = matrix(s), 
                 stringsAsFactors = FALSE)

# Removing the mean from the data frame to only keep quartiles, minimum and maximum.
row.to.remove <- which(df$intervals %in% "Mean")
df <- df[- row.to.remove,] 

# Enlarging range to include minimum and maximum.
v = df$values
v[1] <- floor(v[1])
v[length(v)] <- ceiling(tail(v, 1))

# Rounding to upgrade clarity in graph and table.
for (i in 1:(length(v) - 2))  {
  v[i + 1] <- round(v[i + 1])
}

# Building up data frame with default percentages per collateral quartiles.
df <- df %>% mutate(values = v)
tab <- temp_clean %>% select(y, collateral) %>% 
  mutate(intervals = cut(collateral, breaks = df$values, 
                         include.lowest = TRUE, right = TRUE, dig.lab = 5)) %>%
  group_by(intervals) %>% 
  summarize(n = n(), perc_def = round((1 - (sum(y) / n)) * 100)) %>%  
  select(intervals, n, perc_def) %>% as.data.frame()

# Including default percentage in case of missing values in collateral.
tab_2 <- data.frame(intervals = "NAs")
tab_2 <- tab_2 %>% 
  mutate(n = nrow(temp_na), perc_def =  round((1 - (sum(temp_na$y) / n)) * 100))

# Assembling tab and tab_2
tab_3 <- rbind(tab, tab_2)

# Drawing a geom_bar graph with default percentages 
graph <-  tab_3 %>%
  ggplot(aes(intervals, perc_def)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Default Rate per Collateral Quartile") +
  xlab("Collateral Quartile") + ylab("Default Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
```

<br>

On the one hand, the lowest average default rate relates to under-collateralized loans, i.e. loans with "negative collateral" or in other words with property current market value lower than the sum of the mortgage balance due and the loan amount. One explanation could be that lenders are particularly cautious under such circumstances: from a domain standpoint, it may make sense to extend a loan that is under-collateralized (except in a few states with so-called "anti-deficiency laws") if the lender is extending a "character loan" on the basis of good credit history, capacity to pay and liquid assets that could be recovered in a lawsuit.

On the other hand, loans with missing value about collateral have a rather high average default rate. Of course, we should know the number of loans with missing information about collateral in order to check statistical representativeness: let's have a look at the next table which digitalizes information.

<br>

```{r Table about default rate per collateral quartile}
tab <- tab_3 %>% mutate(
  perc_def = paste(perc_def, "%", sep = " ")) %>%
  rename(Collateral = intervals, Loan_Number = n, Default_Rate = perc_def)
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 14) %>% column_spec(1:3, width = "2in")
rm(temp, s, df, row.to.remove, v, graph, tab, tab_2, tab_3)
```

<br>

The number of loans without information about collateral is 205, which provides statistical representativeness.  

Since collateral seems to have some predictive power, this predictor will be added to the training, test and validation sets.

<br>

```{r Adding collateral predictor to all relevant datasets}
hmeq <- hmeq %>% mutate(collateral = prop_val - mort_due - loan)
hmeq_train <- hmeq_train %>% mutate(collateral = prop_val - mort_due - loan)
hmeq_clean <- hmeq_clean %>% mutate(collateral = prop_val - mort_due - loan)
hmeq_clean_train <- hmeq_clean_train %>% mutate(collateral = prop_val - mort_due - loan)
hmeq_clean_test <- hmeq_clean_test %>% mutate(collateral = prop_val - mort_due - loan)
hmeq_clean_val <- hmeq_clean_val %>% mutate(collateral = prop_val - mort_due - loan)
hmeq_mis <- hmeq_mis %>% mutate(collateral = prop_val - mort_due - loan)
hmeq_mis_train <- hmeq_mis_train %>% mutate(collateral = prop_val - mort_due - loan)
hmeq_mis_test <- hmeq_mis_test %>% mutate(collateral = prop_val - mort_due - loan)
hmeq_mis_val <- hmeq_mis_val %>% mutate(collateral = prop_val - mort_due - loan)
```

<br>

#### 5. Reason for Loan Request

<br>

```{r Graph with default rate per loan reason}

# Preparing data frames.
temp <- hmeq_train %>% select(y, reason) %>% mutate(y = as.numeric(y) - 1) 
temp_clean <- temp[is.na(temp$reason) == 0,] 
temp_na <- temp[is.na(temp$reason) == 1,]

# Building up data frame with default percentages per loan reason.
tab <- temp_clean %>% select(y, reason) %>% group_by(reason) %>% 
  summarize(n = n(), perc_def = round((1 - (sum(y) / n)) * 100)) %>%  
  select(reason, n, perc_def) %>% as.data.frame()

# Including default percentage in case of missing values in loan reason.
tab_2 <- data.frame(reason = "NAs")
tab_2 <- tab_2 %>% 
  mutate(n = nrow(temp_na), perc_def =  round((1 - (sum(temp_na$y) / n)) * 100))

# Assembling tab and tab_2.
tab_3 <- rbind(tab, tab_2) %>% as.data.frame()

# Drawing a geom_bar graph with default percentages. 
graph <-  tab_3 %>%
  ggplot(aes(reason, perc_def)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Default Rate per Loan Reason") +
  xlab("Loan Reason") + ylab("Default Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
```

<br>

Average default rate changes in a limited way according to category. Default rates are digitalized hereinafter.

Less predictive power is expected from this predictor. 

<br>

```{r Table about default rate per loan reason quartile}
tab <- tab_3 %>% mutate(perc_def = paste(perc_def, "%", sep = " ")) %>%
  mutate(n = format(n, big.mark = " ")) %>%
  rename(Loan_Reason = reason, Loan_Number = n, Default_Rate = perc_def)
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 14) %>% column_spec(1:3, width = "2in")
rm(temp, s, df, row.to.remove, v, graph, tab, tab_2, tab_3)
```

<br>

#### 6. Professional Occupation

<br>

```{r Graph about default rate per occupational category}

# Preparing data frames.
temp <- hmeq_train %>% select(y, job) %>% mutate(y = as.numeric(y) - 1) 
temp_clean <- temp[is.na(temp$job) == 0,] 
temp_na <- temp[is.na(temp$job) == 1,]

# Building up data frame with default percentages per occupational category.
tab <- temp_clean %>% select(y, job) %>% group_by(job) %>% 
  summarize(n = n(), perc_def = round((1 - (sum(y) / n)) * 100)) %>%  
  select(job, n, perc_def) %>% as.data.frame()

# Including default percentage in case of missing values in prop_val.
tab_2 <- data.frame(job = "NAs")
tab_2 <- tab_2 %>% 
  mutate(n = nrow(temp_na), perc_def =  round((1 - (sum(temp_na$y) / n)) * 100))

# Assembling tab and tab_2.
tab_3 <- rbind(tab, tab_2) %>% as.data.frame()

# Drawing a geom_bar graph with default percentages. 
graph <-  tab_3 %>%
  ggplot(aes(job, perc_def)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Default Rate per Occupational Category") +
  xlab("Occupational Category") + ylab("Default Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
```

<br>

The average default rate fluctuates substantially per occupational category. Percentages are available in the next table.

<br>

```{r Table about default rate per occupational category}
tab <- tab_3 %>% mutate(perc_def = paste(perc_def, "%", sep = " ")) %>%
  rename(Professional_Occupation = job, Loan_Number = n, Default_Rate = perc_def)
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 14) %>% column_spec(1:3, width = "2in")
rm(temp, s, df, row.to.remove, v, graph, tab, tab_2, tab_3)
```

<br>

Where information is available about professional occupation, the highest default rate is noticed for self employment and the lowest for office professional occupations. Predictive impact is expected from this predictor. 

Missing information is associated with the lowest average default rate: 9 %! From a domain point of view, no explanation is available on the Kaggle website; could it come
from special attention having been paid to borrowers with no identified professional occupation? From a data science point of view, this is most interesting for predicting in case of missing information: when information is missing about professional occupation, "Repaid" could be predicted for all loans with expected sensitivity and precision of 91 %, on condition, of course, that the 9 % default rate has statistical representativeness, which can be assumed with 92 observations!

<br>

#### 7. Number of Years in Current Professional Occupation

<br>

```{r Graph about default rate per approximate quartile of job_years}

# Preparing data frames.
temp <- hmeq_train %>% select(y, job_years) %>% mutate(y = as.numeric(y) - 1) 
temp_clean <- temp[is.na(temp$job_years) == 0,] 
temp_na <- temp[is.na(temp$job_years) == 1,]

# Calculating job_years quartiles.
s <- summary(temp_clean$job_years)
df <- data.frame(intervals = as.character(names(s)), values = matrix(s), 
                 stringsAsFactors = FALSE)

# Removing the mean from the data frame to only keep quartiles, minimum and maximum.
row.to.remove <- which(df$intervals %in% "Mean")
df <- df[- row.to.remove,] 

# Enlarging range to include minimum and maximum.
v = df$values
v[1] <- floor(v[1])
v[length(v)] <- ceiling(tail(v, 1))

# Rounding to upgrade clarity in graph and table.
for (i in 1:(length(v) - 2))  {
  v[i + 1] <- round(v[i + 1])
}

# Building up data frame with default percentages per job_years quartile.
df <- df %>% mutate(values = v)
tab <- temp_clean %>% select(y, job_years) %>% 
  mutate(intervals = cut(job_years, breaks = df$values, 
                        include.lowest = TRUE, right = TRUE, dig.lab = 5)) %>%
  group_by(intervals) %>% 
  summarize(n = n(), perc_def = round((1 - (sum(y) / n)) * 100)) %>% 
  select(intervals, n, perc_def) %>% as.data.frame()

# Including default percentage in case of missing values in job_years.
tab_2 <- data.frame(intervals = "NAs")
tab_2 <- tab_2 %>% 
  mutate(n = nrow(temp_na), perc_def =  round((1 - (sum(temp_na$y) / n)) * 100))

# Assembling tab and tab_2.
tab_3 <- rbind(tab, tab_2) %>% as.data.frame()

# Drawing a geom_bar graph with default percentages.
graph <-  tab_3 %>%
  ggplot(aes(intervals, perc_def)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Default Rate per Number of Years in Current Job") +
  xlab("Number of Years in Current Job") + ylab("Default Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
```

<br>

Average default rate varies rather slightly according to the number of years in the current professional occupation. This delivers no great expectations about predictiveness. But there is a slight downward trend which makes sense. 

In case of missing values, there is a great piece of information: the average default rate is only 10 % (please see table below). This would allow, in case of missing information about the number of years, to predict "Repaid" for all loans while expecting sensitivity and precision of 10 %, especially so since, with 166 observations, a decent level of statistical representativeness is provided! 

<br>

```{r Table about default rate per job_years quartile}
tab <- tab_3 %>% mutate(perc_def = paste(perc_def, "%", sep = " ")) %>%
  rename(Job_Years = intervals, Loan_Number = n, Default_Rate = perc_def)
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 14) %>% column_spec(1:3, width = "2in")
rm(temp, s, df, row.to.remove, v, graph, tab, tab_2, tab_3)
```

<br>

#### 8. Number of Major Derogatory Reports

<br>

Lets' evaluate the predictive power of the number of major derogatory reports.

<br>

```{r Graph of default rate per quartile of number of major derogatory reports}

# Wrangling data.
temp <- hmeq_train %>% mutate(y = as.numeric(y) - 1) %>% select(y, derog)
temp_clean <- temp[is.na(temp$derog) == 0,] 
temp_na <- temp[is.na(temp$derog) == 1,]

# Building up data frame with default percentages when there is no missing value.
tab <- temp_clean %>% group_by(derog) %>% 
  summarize(n = n(), perc_def = round((1 - (sum(y) / n)) * 100)) %>% 
  select(derog, n, perc_def) %>% as.data.frame()

# Including default rate in case of missing values about the number of major derogatory reports.
tab_2 <- data.frame(derog = "NAs")
tab_2 <- tab_2 %>% 
  mutate(n = nrow(temp_na), perc_def =  round((1 - (sum(temp_na$y) / n)) * 100))

# Assembling tab and tab_2.
tab_3 <- rbind(tab, tab_2) %>% as.data.frame()

# Changing type of data in derog predictor: factors instead of integers. Objective of this change: getting bars easily in ascending order in the next graph.
v <- unique(tab_3$derog)
v <- v[!v == "NAs"]
v <- as.numeric(v)
tab_3 <- tab_3 %>% mutate(derog = factor(tab_3$derog, levels = v))

# Drawing a geom_bar graph with default percentages. 
graph <-  tab_3 %>%
  ggplot(aes(derog, perc_def)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Default Rate per Number of Major Derogatory Reports") +
  xlab("Number of Major Derogatory Reports") + ylab("Default Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
```

<br>

There is vast variation in average default rate according to the number of major derogatory reports. 

Where there is no major derogatory report, the default rate is very limited - 16 % as indicated in the next table - which is in accordance with expectations; it is all the more interesting that the rate of 16 % has high statistical representativeness since it relates to 1,524 cases! 

When the number of reports increases, the default rate follows an upward trend, a bit jumpy though, and from 7 to 11 reports, the default rate gets stuck to 100 %. This looks like a very strong predictor of default.

Where there is no information available about the number of major derogatory reports, surprisingly enough the default rate is the lowest, being even lower than where there is no report at all! It is all the more surprising that this very low percentage has statistical representativeness with 217 cases as indicated in the next table. This very low default rate can be a powerful predictor of "Repaid" in case of missing values! 

<br>

```{r Table about default rate per number of major derogatory reports}
tab <- tab_3 %>% mutate(
  perc_def = paste(perc_def, "%", sep = " ")) %>%
  mutate(n = format(n, big.mark = " ")) %>%
  rename(Nr_Major_Derogatory_Reports = derog, Loan_Number = n, Default_Rate = perc_def)
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 14) %>% column_spec(1:3, width = "2in")
rm(temp, s, df, row.to.remove, v, graph, tab, tab_2, tab_3)
```

<br>

#### 9. Number of Delinquent Lines

<br>

```{r Graph of default rate per number of delinquent lines}

# Wrangling data.
temp <- hmeq_train %>% mutate(y = as.numeric(y) - 1) %>% select(y, delinq)
temp_clean <- temp[is.na(temp$delinq) == 0,] 
temp_na <- temp[is.na(temp$delinq) == 1,]

# Building up data frame with default percentages when there is no missing value.
tab <- temp_clean %>% group_by(delinq) %>% 
  summarize(n = n(), perc_def = round((1 - (sum(y) / n)) * 100)) %>% 
  select(delinq, n, perc_def) %>% as.data.frame()

# Including default rate in case of missing values.
tab_2 <- data.frame(delinq = "NAs")
tab_2 <- tab_2 %>% 
  mutate(n = nrow(temp_na), perc_def =  round((1 - (sum(temp_na$y) / n)) * 100))

# Assembling tab and tab_2.
tab_3 <- rbind(tab, tab_2) %>% as.data.frame()

# Changing type of data in delinq predictor: factors instead of integers. Objective of this change: getting bars easily in ascending order in the next graph.
v <- unique(tab_3$delinq)
v <- v[!v == "NAs"]
v <- as.numeric(v)
tab_3 <- tab_3 %>% mutate(delinq = factor(tab_3$delinq, levels = v))

# Drawing a geom_bar graph with default percentages. 
graph <-  tab_3 %>%
  ggplot(aes(delinq, perc_def)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Default Rate per Number of Delinquent Lines") +
  xlab("Number of Delinquent Lines") + ylab("Default Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
```

<br>

The graph looks very much like the previous one about the number of major derogatory reports:

- there is an upward trend in default rate across loans ranked according to the number of delinquent lines;
- when information about the number of delinquent lines is missing, default rate is low, i.e. 13 % as indicated in the next table. 

The main difference is that the upward trend is monotonically increasing: the default rate never decreases when the number of delinquent lines increases. 

<br>

```{r Table about default rate per number of delinquent lines}
tab <- tab_3 %>% mutate(perc_def = paste(perc_def, "%", sep = " ")) %>%
  mutate(n = format(n, big.mark = " ")) %>%
  rename(Nr_Delinquent_Lines = delinq, Loan_Number = n, Default_Rate = perc_def)
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 14) %>% column_spec(1:3, width = "2in")
rm(temp, s, df, row.to.remove, v, graph, tab, tab_2, tab_3)
```

<br>

#### 10. The Oldest Trade Line in Months

<br>

```{r Graph about default rate per quartile of oldest trade line in months}

# Preparing data frames.
temp <- hmeq_train %>% select(y, oldest_trade) %>% mutate(y = as.numeric(y) - 1) 
temp_clean <- temp[is.na(temp$oldest_trade) == 0,] 
temp_na <- temp[is.na(temp$oldest_trade) == 1,]

# Calculating quartiles of oldest trade line in months.
s <- summary(temp_clean$oldest_trade)
df <- data.frame(intervals = as.character(names(s)), values = matrix(s), 
                 stringsAsFactors = FALSE) 

# Removing the mean from the data frame to only keep quartiles, minimum and maximum.
row.to.remove <- which(df$intervals %in% "Mean")
df <- df[- row.to.remove,] 

# Enlarging range to include minimum and maximum.
v = df$values
v[1] <- floor(v[1])
v[length(v)] <- ceiling(tail(v, 1))

# Rounding to upgrade clarity in graph and table.
for (i in 1:(length(v) - 2))  {
  v[i + 1] <- round(v[i + 1])
}

# Building up data frame with default percentages per quartile of the oldest trade line in months.
df <- df %>% mutate(values = v)
tab <- temp_clean %>% select(y, oldest_trade) %>% 
  mutate(intervals = cut(oldest_trade, breaks = df$values, 
                        include.lowest = TRUE, right = TRUE, dig.lab = 5)) %>%
  group_by(intervals) %>% 
  summarize(n = n(), perc_def = round((1 - (sum(y) / n)) * 100)) %>%
  select(intervals, n, perc_def) %>% as.data.frame()

# Including default percentage in case of missing values about the oldest trade line in months.
tab_2 <- data.frame(intervals = "NAs")
tab_2 <- tab_2 %>% 
  mutate(n = nrow(temp_na), perc_def =  round((1 - (sum(temp_na$y) / n)) * 100))

# Assembling tab and tab_2
tab_3 <- rbind(tab, tab_2) %>% as.data.frame()

# Drawing a geom_bar graph with default percentages. 
graph <-  tab_3 %>%
  ggplot(aes(intervals, perc_def)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Default Rate per Quartile of Oldest Trade Line in Months") +
  xlab("Quartile of Oldest Trade Line in Months") + ylab("Default Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
```

<br>

There is a substantial difference in average default rate between the first and the fourth interval: as displayed in the next table, in the first interval, default rate is 29 % while it falls to 12 % in the fourth interval. It looks like a rather promising predictor of default.

<br>

```{r Table about default rate per quartile of oldest trade}
tab <- tab_3 %>% mutate(perc_def = paste(perc_def, "%", sep = " ")) %>%
  rename(Oldest_Trade_Line_in_Months = intervals, Loan_Number = n, Default_Rate = perc_def)
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 14) %>% column_spec(1:3, width = "2in")
rm(temp, s, df, row.to.remove, v, graph, tab, tab_2, tab_3)
```

<br>

#### 11. Number of Recent Credits

<br>

Lets' evaluate the predictive power of the number of recent credits.

<br>

```{r Graph with default rates according to number of recent credits}

# Wrangling data.
temp <- hmeq_train %>% mutate(y = as.numeric(y) - 1) %>% select(y, recent_cred)
temp_clean <- temp[is.na(temp$recent_cred) == 0,] 
temp_na <- temp[is.na(temp$recent_cred) == 1,]

# Building up data frame with default percentages where there is no missing value about number of recent credits.
tab <- temp_clean %>% group_by(recent_cred) %>% 
  summarize(n = n(), perc_def = round((1 - (sum(y) / n)) * 100)) %>% 
  select(recent_cred, n, perc_def) %>% as.data.frame()

# Including default rate in case of missing values in number of recent credits.
tab_2 <- data.frame(recent_cred = "NAs")
tab_2 <- tab_2 %>% 
  mutate(n = nrow(temp_na), perc_def =  round((1 - (sum(temp_na$y) / n)) * 100))

# Assembling tab and tab_2.
tab_3 <- rbind(tab, tab_2) %>% as.data.frame()

# Managing factors to easily have numbers of recent credits in "consecutive" order in the next graph.
v <- unique(tab_3$recent_cred)
v <- v[!v == "NAs"]
v <- as.numeric(v)
tab_3 <- tab_3 %>% mutate(recent_cred = factor(tab_3$recent_cred, levels = v))

# Drawing a geom_bar graph with default percentages. 
graph <-  tab_3 %>%
  ggplot(aes(recent_cred, perc_def)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Default Rate per Number of Recent Credits") +
  xlab("Number of Recent Credits") + ylab("Default Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
```

<br>

There is an upward trend in the average default rate per number of recent credits, even if both 100 % rates are ignored because they only relate altogether to 2 observations! This upward trend could be an effective predictor.

On the subset without information about the number of recent credits, the average default rate is very low, i.e. 15 % as indicated in the next table. This is a powerful predictor of "Repaid" in case of missing information: on the training set, predicting "Repaid" when information is missing about the number of recent credits gives 85 % for both sensitivity and precision, which is much higher than the global 75-50 target! 

<br>

```{r Table about default rate per number of recent credits}
tab <- tab_3 %>% mutate(
  perc_def = paste(perc_def, "%", sep = " ")) %>%
  rename(Nr_of_Recent_Credits = recent_cred, Loan_Number = n, Default_Rate = perc_def)
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 14) %>% column_spec(1:3, width = "2in")
rm(temp, s, df, row.to.remove, v, graph, tab, tab_2, tab_3)
```

<br>

#### 12. Number of Credits

<br>

Lets' evaluate the predictive power of the number of credits.

<br>

```{r Graph about default rate approximately per approximate quartile of number of credits}

# Preparing data frames.
temp <- hmeq_train %>% select(y, credits) %>% mutate(y = as.numeric(y) - 1) 
temp_clean <- temp[is.na(temp$credits) == 0,] 
temp_na <- temp[is.na(temp$credits) == 1,]

# Calculating quartiles of the number of credits.
s <- summary(temp_clean$credits)
df <- data.frame(intervals = as.character(names(s)), values = matrix(s), 
                 stringsAsFactors = FALSE)

# Removing the mean from the data frame to only keep quartiles, minimum and maximum.
row.to.remove <- which(df$intervals %in% "Mean")
df <- df[- row.to.remove,] 

# Building up data frame with percentages of default per approximate quartile of number of credits.
tab <- temp_clean %>% select(y, credits) %>% 
  mutate(intervals = cut(credits, breaks = df$values, 
                         include.lowest = TRUE, right = TRUE, dig.lab = 5)) %>%
  group_by(intervals) %>% 
  summarize(n = n(), perc_def = round((1 - (sum(y) / n)) * 100)) %>%
  select(intervals, n, perc_def) %>% as.data.frame()

# Including default percentage in case of missing values in the number of credits.
tab_2 <- data.frame(intervals = "NAs")
tab_2 <- tab_2 %>% 
  mutate(n = nrow(temp_na), perc_def =  round((1 - (sum(temp_na$y) / n)) * 100))

# Assembling tab and tab_2.
tab_3 <- rbind(tab, tab_2) %>% as.data.frame()

# Drawing a geom_bar graph with default percentages. 
graph <-  tab_3 %>%
  ggplot(aes(intervals, perc_def)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Default Rate per Number of Credits") +
  xlab("Number of Credits") + ylab("Default Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
```

<br>

Average default rate varies in a rather limited way.

<br>

```{r Table about default rate per approximate quartile of credits}
tab <- tab_3 %>% mutate(
  perc_def = paste(perc_def, "%", sep = " ")) %>%
  rename(Number_of_Credits = intervals, Loan_Number = n, Default_Rate = perc_def)
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 14) %>% column_spec(1:3, width = "2in")
rm(temp, s, df, row.to.remove, v, graph, tab, tab_2, tab_3)
```

<br>

#### 13. Debt-to-income Ratio

<br>

```{r Graph about default rate per debt_to_inc quartile}

# Preparing data frames.
temp <- hmeq_train %>% select(y, debt_to_inc) %>% mutate(y = as.numeric(y) - 1) 
temp_clean <- temp[is.na(temp$debt_to_inc) == 0,] 
temp_na <- temp[is.na(temp$debt_to_inc) == 1,]

# Calculating debt_to_inc quartiles.
s <- summary(temp_clean$debt_to_inc)
df <- data.frame(intervals = as.character(names(s)), values = matrix(s), 
                 stringsAsFactors = FALSE)

# Removing the mean from the data frame to only keep quartiles, minimum and maximum.
row.to.remove <- which(df$intervals %in% "Mean")
df <- df[- row.to.remove,] 

# Enlarging range to include minimum and maximum.
v = df$values
v[1] <- floor(v[1])
v[length(v)] <- ceiling(tail(v, 1))

# Rounding to impprove clarity in graph.
for (i in 1:(length(v) - 2))  {
  v[i + 1] <- round(v[i + 1])
}

# Building up data frame with default percentages per debt-to-inc quartile.
df <- df %>% mutate(values = v)
tab <- temp_clean %>% select(y, debt_to_inc) %>% 
  mutate(intervals = cut(debt_to_inc, breaks = df$values, 
                        include.lowest = TRUE, right = TRUE, dig.lab = 5)) %>%
  group_by(intervals) %>% 
  summarize(n = n(), perc_def = round((1 - (sum(y) / n)) * 100)) %>%
  select(intervals, n, perc_def) %>% as.data.frame()

# Including default percentage in case of missing values in mort_due
tab_2 <- data.frame(intervals = "NAs")
tab_2 <- tab_2 %>% 
  mutate(n = nrow(temp_na), perc_def =  round((1 - (sum(temp_na$y) / n)) * 100))

# Assembling tab and tab_2.
tab_3 <- rbind(tab, tab_2)

# Drawing a geom_bar graph with default percentages.
graph <-  tab_3 %>%
  ggplot(aes(intervals, perc_def)) + 
  geom_bar(stat = "identity", width = 0.40, color = "#007ba7", fill = "#9bc4e2") + 
  ggtitle("Default Rate per Debt-to-income Ratio") +
  xlab("Debt-to-income Ratio") + ylab("Default Rate (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(angle = 45, hjust = 1, size = 12),
        axis.text.y = element_text(size = 12))
graph
```

<br>

In the fourth interval, the average default rate is more than twice as large as in other intervals, which is not at all surprising. 

In case of missing values, the average default rate is as high as 61 %, which can be most valuable information to predict.

<br>

```{r Table about default rate per debt-to-income ratio quartile}
tab <- tab_3 %>% mutate(
  perc_def = paste(perc_def, "%", sep = " ")) %>%
  rename(Debt_to_Income_Ratio = intervals, Loan_Number = n, Default_Rate = perc_def)
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 14) %>% column_spec(1:3, width = "2in")
rm(temp, s, df, row.to.remove, v, graph, tab, tab_2, tab_3)
```

<br>

### B. Insights from EDA

#### 1. Insight into Predictor Content

<br>

In EDA, predictor categories have been associated with average default rates. 

For some predictors, category default rates can be substantially different. In the case of professional occupation, average default rates per category vary from 12 % to 40 %. In the instance of the number of major derogatory reports, variation is between 16 % and more than 60 % when there are more than 2 major derogatory reports. Concerning the number of delinquent lines, variation is between 13 % and more than 60 % when there are more than 2 delinquent lines. Regarding the number of recent credits, variation is between 15 % and more than 50 % when there are more than 5 reports. These ranges do not include average default rates corresponding to missing values. 

In the case of other predictors, variation is rather limited: 19-22 in the instance of the reason of the loan request, 18-23 for the number of years in the current professional occupation and 16-24 for the number of credits. 

A priori, larger variation can boost predictive power of the predictor, if, of course, the number of observations per category permits statistical representativeness. Modeling on the dataset with full information on all predictors will aim at benefiting from this piece of information. 

<br> 

#### 2. Insight into Loans with Incomplete Information

<br>

The EDA has shown  an interesting pattern about the average default rate associated with missing information: that rate can vary according to predictor. Let's retrieve a few extreme examples:

- when information is missing about the property current market value or about the debt-to-income ratio then the average default rate is respectively 96 % or 61 %;

- when information is missing about the professional occupation or about the number of years in the current professional occupation then the average default rate falls at 9 % or 10 %. 

This seems to be a very valuable insight which will be extensively used when modeling on the dataset with incomplete information.

<br>

## V. Modeling, Analysis and Insights in the case of Loans with Full Information

### A. Individual Models

#### 1. Selecting and Training Individual Models

<br>

This is a machine learning part of the project. 

21 models have been pre-tested with the train() function from caret and with the predict() function. All models have been trained on the training set with full information (hmeq_clean_train) and used for prediction on the test set with full information (hmeq_clean_test). Here is the list, with the model names as they stand in the caret package (http://topepo.github.io/caret/index.html): adaboost, gam, gamboost, gamLoess, gbm, glm, knn, lda, monmlp, naive_bayes, qda, Rborist, rf, rpart, svmLinear, svmRadialCost, svmRadialSigma, wsrf, xgbDART, xgbLinear, xgbTree. 

Model performance has been evaluated on the basis of sensitivity and precision. 6 models have emerged: adaboost, gam, gamLoess, rf, wsrf and xgbLinear. For these 6 models, performance has been summarized in the table below. 

<br>

```{r Training models on hmeq_clean_train}

# Training  the 6 selected models on hmeq_clean_train.
models <- c("adaboost", "gam", "gamLoess", "rf", "wsrf", "xgbLinear")
fits <- lapply(models, function(model){
  set.seed(1)
  train(y ~ ., method = model, data = hmeq_clean_train)
  }) 
names(fits) <- models
len_mod <- length(models)

# Getting fitted values from all models.
fitted <- sapply(fits, function(object) predict(object)) %>% as.data.frame()

# Getting sensitivity and precision from all models.
seq_mod <- 1:len_mod
sensitivities <- sapply(seq_mod, function(i) 
  sensitivity(as.factor(fitted[, i]), hmeq_clean_train$y))
posPredValues <- sapply(seq_mod, function(i) 
  posPredValue(as.factor(fitted[, i]), hmeq_clean_train$y))

# Expressing in percentage points and rounding.
sensitivities <- round(sensitivities * 100)
posPredValues <- round(posPredValues * 100)

# Building up a table with sensitivity and precision from all models.
tab <- data.frame(Model = models, 
                  Sensitivity = sensitivities, Precision = posPredValues)
tab <- tab %>% mutate(Sensitivity = paste(Sensitivity, "%", sep = " ")) %>%
  mutate(Precision = paste(Precision, "%", sep = " ")) %>%
  as.data.frame()
              
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 14) %>% column_spec(1:3, width = "1.5in")

rm(sensitivities, posPredValues, tab)
```

<br>

#### 2. Insights from Training Individual Models

<br>

Sensitivity and precision are equal to 100 % or very close to 100 % for adaboost, rf, wsrf and xgbLinear. This looks like overfitting. The same level cannot be expected on the test set. 

Before calculating sensitivity and precision on the test set, let's have a look at broader statistics. This will be done for xgbLinear, since eXtreme Gradient Boosting models have  been very much acclaimed (please see https://towardsdatascience.com/https-medium-com-vishalmorde-xgboost-algorithm-long-she-may-rein-edd9f99be63d ).

<br> 

```{r Confusion matrix and other statistics from model xgbLinear}
confusionMatrix(fitted[, 6], hmeq_clean_train$y)
rm(fitted)
```

<br>

This illustrates the overfitting diagnosis: accuracy, sensitivity, specificity, positive predictive value, negative predictive value and balanced accuracy are all equal to 100 %. 

On the contrary, for models gam and gamLoess, precision is a bit lower and sensitivity is substantially lower. Since models adaboost, rf, wsrf and xgbLinear are probably overfitting and since gam and gamLoess do not show the same pattern, gam and gamLoess will remain preselected. 

<br>

#### 3. Predicting with Individual Models on the Test Set

<br>

Now let's turn to sensitivity and precision on the test set for loans with full information (hmeq_clean_test).

<br>

```{r Predicting on test set and getting sensitivity and precision from the 6 preselected models}

# Getting predictions on dataset hmeq_clean_test.
pred_test <- sapply(fits, function(object) 
  predict(object, newdata = hmeq_clean_test)) %>% as.data.frame()

# Getting sensitivity and precision from all models on test set.
pred_test_sensitivities <- sapply(seq_mod, function(i) 
  sensitivity(as.factor(pred_test[, i]), hmeq_clean_test$y))
pred_test_posPredValues <- sapply(seq_mod, function(i) 
  posPredValue(as.factor(pred_test[, i]), hmeq_clean_test$y))

# Expressing in percentage points and rounding.
pred_test_sensitivities <- round(pred_test_sensitivities * 100)
pred_test_posPredValues <- round(pred_test_posPredValues * 100)

# Printing table.
tab <- data.frame(Model = models, Sensitivity = pred_test_sensitivities, 
                  Precision = pred_test_posPredValues)
tab <- tab %>% mutate(Sensitivity = paste(Sensitivity, "%", sep = " ")) %>%
  mutate(Precision = paste(Precision, "%", sep = " ")) %>%
  as.data.frame()
              
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 14) %>% column_spec(1:3, width = "2in")

rm(pred_test, pred_test_sensitivities, pred_test_posPredValues, tab)
```

<br>

On the training set, 4 models had sensitivity and precision of 100 % or almost 100 %: adaboost, rf, wsrf and xgbLinear. On the test set, sensitivity has sunk around 40 %. This confirms the overfitting diagnosis. Annoyingly enough, sensitivity level is now much lower than the global target of 75! Precision hovers between 66 % and 89 %, which remains higher than the target of 50 %.

2 models had more modest performance levels on the training set, i.e. gam and gamLoess. They almost maintain the same level on the test set, which is not so surprising since there was no impression of overfitting on the training set. Moreover, gamLoess seems competitive now, hitting respectively 39 % and 89 % in sensitivity and precision, which compares to wsrf performance of 40-66! 

Now the main challenge is sensitivity: jumping from around 40 % to at least 75 %! 

<br>

#### 4. Insights from Predicting on the Test Set

<br>

Actually, there is some leeway since precision is above target and since we know that there is a trade-off between sensitivity and precision. Indeed, these performance levels have been obtained by predicting "Default" when probability of "Default"" was larger than 50 %; if the probability threshold is lowered, sensitivity can hopefully get leveraged, of course to the detriment of precision. But since precision is above the global target, there is some room for manoeuvre! Let's use it!

Practically, probability threshold will be computed for the 6 models and for probability thresholds ranging from 0.05 to 0.5 (standard threshold) with increments of 0.001. So, sensitivity and precision will be calculated for the 6 models and for 451 probability thresholds. For the sake of clarity, a first table will provide sensitivity and precision for the 6 models and for a limited number of probability thresholds. The wider picture will be provided in a graph later on. 

<br>

#### 5. Boosting Performance by Fine-tuning Probability Thresholds

<br>

```{r Getting probabilities on test set - tuning probability threshold - result table}

# Getting probabilities from predict(), instead of outcomes.
prob_test <- lapply(fits, function(object) 
  predict(object, newdata = hmeq_clean_test, type = "prob"))

# Extracting for each model only the first column of probabilities, which is 
# the column of probabilities of default. This means extracting from prob_test 
# columns with odd position number. 
seq <- seq(1, (len_mod * 2) - 1, 2)
prob_test <- prob_test %>% as.data.frame() %>% select(seq)
names(prob_test) <- gsub(".Default", "", names(prob_test))
rm(seq)

# Generating sequence of probability thresholds. 
cutoffs <- seq(0.05, 0.5, 0.001)
len_cut <- length(cutoffs)
seq_cut <- 1:len_cut

# Creating storage facilities (data frames) as receptacles of all sensitivity and precision measurements from all combinations models/thresholds (test set).
test_sensitivities <- data.frame(matrix(nrow = len_cut, ncol = len_mod) * 1)
names(test_sensitivities) <- models
test_posPredValues <- data.frame(matrix(nrow = len_cut, ncol = len_mod) * 1)
names(test_posPredValues) <- models

# Storing all sensitivity and precision measurements from all combinations models/thresholds (test set).
for (i in seq_cut) {
  for (j in seq_mod) { 
    refitted <- ifelse(prob_test[, j] > cutoffs[i], "Default", "Repaid") %>%
      factor(levels = levels(hmeq_clean_test$y))
    test_sensitivities[i, j] <- confusionMatrix(refitted, hmeq_clean_test$y)$byClass[1]
    test_posPredValues[i, j] <- confusionMatrix(refitted, hmeq_clean_test$y)$byClass[3]
  } 
}
rm(i, j, refitted)

# Expressing sensitivity and precision in percentage points and rounding.
test_sensitivities <- round(test_sensitivities * 100)
test_posPredValues <- round(test_posPredValues * 100)

# Creating one single data frame to unify the sensitivity data frame and the precision data frame. 2 columns are allocated to each model, the first one being destined for sensitivity levels for all probability thresholds, the second one being destined for precision. 
evaluation_table_test_all <- 
  data.frame(matrix(nrow = len_cut, ncol = len_mod * 2) * 1)

# Naming columns after model, with "s" for sensitivity and "p" for precision. 
seq_odd <- seq(1, (len_mod * 2) - 1, 2)
colnames(evaluation_table_test_all)[seq_odd] <- c(paste(models, "s", sep = "_"))
seq_even <- seq(2, (len_mod * 2) , 2)
colnames(evaluation_table_test_all)[seq_even] <- c(paste(models, "p", sep = "_"))
rm(seq_odd, seq_even)

# Importing sensitivity and precision measurements from all models and thresholds.
for (i in 1:len_mod) {
  evaluation_table_test_all[(i * 2) - 1] <- test_sensitivities[, i]
  evaluation_table_test_all[(i * 2)] <- test_posPredValues[, i]
} 
rm(i, test_sensitivities, test_posPredValues)

# Adding cutoff column.
evaluation_table_test_all <- evaluation_table_test_all %>%
  mutate(Thresh = as.character(round(cutoffs, 3))) %>% 
  select(Thresh, everything())

# Printing part of the table: one out of 10 threshold values.
seq <- seq(1, len_cut, 10)
tab <- evaluation_table_test_all[seq, ] %>% 
  mutate(Thresh = format(Thresh, digits = 4, nsmall = 3)) %>% as.data.frame()

kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 16) 
rm(seq, tab)
```

<br> 

There are some most interesting results in the table above. 

adaboost culminates in sensitivity-precision combinations such as 74-50, which is pretty close to the 75-50 global target, and 69-67. rf and wsrf also produce some solid results such as respectively 72-50 and 69-50. 

Previously, with only the standard probability threshold of 0.5, the best sensitivity-precision result came from the rf model with 42 % for sensitivity and 79 % for precision, which was far away from the global target for sensitivity and well above for precision.

Now, alternative probability thresholds have provided sensitivity-precision results much closer to the global target.

Let's have a look at a graph that summarizes available information and provides a broader picture. 

<br>

```{r Graph with sensitivity-precision performance curves for all models on the test set}
graph <- evaluation_table_test_all %>% ggplot() + 
  geom_point(aes(x = adaboost_s, y = adaboost_p), color = "magenta", size = 1) +
  geom_point(aes(x = gam_s, y = gam_p), color = "yellow", size = 1) + 
  geom_point(aes(x = gamLoess_s, y = gamLoess_p), color = "blue", size = 1) +
  geom_point(aes(x = rf_s, y = rf_p), color = "green", size = 1) +
  geom_point(aes(x = wsrf_s, y = wsrf_p), color = "red", size = 1) +
  geom_point(aes(x = xgbLinear_s, y = xgbLinear_p), color = "brown", size = 1) +
  geom_rect(aes(xmin = 75, xmax = 100, ymin = 50, ymax = 100), 
        fill = "#9bc4e2", alpha = 0.01) +
  ggtitle("Performance Curves on Test Set") + 
  xlim(0, 105) + xlab("Sensitivity (recall) (%)") + 
  ylim(0, 105) + ylab("Precision (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(size = 12), axis.text.y =element_text(size = 12))
graph

# Printing separate legend. 
tab <- data.frame(matrix(models, nrow = 1, ncol = length(models)))
names(tab) <- NULL
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 16) %>% 
  column_spec(1, color = "magenta", background = "lightgrey", bold = TRUE) %>%
  column_spec(2, color = "yellow", background = "lightgrey", bold = TRUE) %>%
  column_spec(3, color = "blue", background = "lightgrey", bold = TRUE) %>%
  column_spec(4, color = "green", background = "lightgrey", bold = TRUE) %>%
  column_spec(5, color = "red", background = "lightgrey", bold = TRUE) %>%
  column_spec(6, color = "brown", background = "lightgrey", bold = TRUE)

rm(graph)
```

<br> 

In the graph above, a sensitivity-precision performance curve is allocated to each model. "recall" has been added as a synonym for "sensitivity" because data science literature often refers to precision-recall curves. 

For each model, the precision-sensitivity curve gives from right to left all the precision-sensitivity combinations corresponding to probability thresholds moving from 0.5 to 0.05. 

The upper-right rectangle, colored in light blue, represents the target area, i.e. the area with sensitivity-precision combinations being equal to the 75-50 global target or superior to it (e.g. 75-55 but not 74-55). 

This graph allows us to rank the models on the test set with respect to the 75-50 benchmark: adaboost comes first, followed by rf, wsrf, xgbLinear; gam and gamLoess are further away, more or less at the same distance from the target rectangle. 

There remains one question, which requires a closer look at result numbers: is adaboost reaching the target? A closer look should be given at results, for the 451 probability thresholds. Especially to check whether intermediate values of the probability threshold would allow adaboost to reach at least the 75-50 benchmark.

<br>

```{r Closer look at results from adaboost}
seq <- seq(201, 211, 1)
tab <- evaluation_table_test_all[seq, ] %>% 
  mutate(Thresh = format(Thresh, digits = 4, nsmall = 3)) %>% as.data.frame()

kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 16) 
rm(seq, tab)
```

<br>

Not very surprisingly, when taking into account intermediate values of the probability threshold, the adaboost model reaches the target, by little it is true: two combinations reach the target, i.e. 76-50 and 75-50, corresponding to respectively values of 0.258 and 0.259 of the probability threshold. 

Three caveats have to be clearly issued.

<br>

#### 6. Caveats about Boosted Results

<br>

First, performance results have been obtained on the test set, which is just a sample, i.e. one third of the dataset with full information. From a statistical point of view, taking into account standard error, (slightly) different results are probable on another sample, on e.g. the validation set. Of course, differences can go both sides. 

Second, encouraging results have been "snatched", after several trials and errors, on a test set that the models already "know". Training that has already been conducted on the test set probably exercises a flattering effect on results, flattering effect that we might miss on the validation set. It would be advisable to get some solid bonus on top of the global target of 75-50. 

Third, performance results have been calculated on loans with full information, i.e. on the dataset hmeq_clean_test. But the global target relates to both loans with and without missing values. Results on both hmeq_clean_val and hmeq_mis_val have to be combined in order to check whether they altogether meet the global target of 75-50. This is an additional reason to boost results on hmeq_clean_test set well above target, if possible.

<br>

#### 7. Insight for Further Performance Boosting

<br>

Taking into account the three caveats, leeway has to be created. An ensemble model will be built up based on the best 3 models (adaboost, rf and wsrf). 

The ensemble model will be built up by majority vote among the best 3 models, taking for each observation from the dataset hmeq_clean_test the outcome that rallies at least 2 votes.

<br>

### B. First Ensemble Model by Majority Vote

#### 1. Methodology

<br>

adaboost, rf and wsrf have been selected to build up an ensemble model because they better performed on the test set, at least with respect to the global target. 

For each probability threshold between 0.05 and 0.5, for each observation from the test set hmeq_clean_test with full information, a majority vote will be organized: for each threshold, the predicted value for a loan is "Default" if at least two out of the three models give for that loan a probability of "Default" larger than the threshold. 

Consequently, the code will generate 451 series of 1,121 predicted values since there are 451 thresholds values between 0.05 and 0.5 with increments of 0.001 and 1,121 rows in hmeq_clean_test. For each series of predicted values, i.e. for each threshold, the sensitivity-precision combination will be calculated. Each sensitivity-precision combination will be one point of the precision-sensitivity curve for the first ensemble model. 

<br>

#### 2. Results from Predicting on the Test Set

<br>

Here is a table of results comparing performance from the best performing individual models (adaboost, rf and wsrf) and from the first ensemble model based on majority vote.  

<br>

```{r First ensemble model - predicting outcomes by majority vote for all thresholds - calculating sensitivity and precision for all thresholds - producing comparative table with the first ensemble model, adaboost, rf and wsrf for a selection of thresholds}

# Building up storage facility.
evaluation_table_test_ensemble <- data.frame(Thresh = cutoffs, ens_s = 1:len_cut,        ens_p = 1:len_cut, stringsAsFactors = FALSE)
evaluation_table_test_ensemble <- evaluation_table_test_ensemble %>%
  mutate(Thresh = as.character(round(cutoffs, 3)))

# For all thresholds, this for loop predicts outcomes from majority vote and calculates  sensitivity and precision.
for (i in 1:len_cut) {
  dummy <- prob_test %>% select(c("adaboost", "rf", "wsrf"))
  dummy[dummy > cutoffs[i]] <- 1
  dummy[dummy <= cutoffs[i]] <- 0
  dummy <- dummy %>% as.data.frame()
  votes <- rowSums(dummy)
  votes <- ifelse(votes >= 2, "Default", "Repaid") %>% 
    factor(levels = levels(hmeq_clean_test$y))
  
  sensitivity <- confusionMatrix(votes, hmeq_clean_test$y)$byClass[1]
  evaluation_table_test_ensemble[i, 2] <-  round(sensitivity * 100)
  
  precision <- confusionMatrix(votes, hmeq_clean_test$y)$byClass[3]
  evaluation_table_test_ensemble[i, 3] <-  round(precision * 100)
} 

# Table of results with the best 3 individual methods and the ensemble model 
evaluation_table_test_global <- evaluation_table_test_all %>% 
  mutate(ens_s = evaluation_table_test_ensemble$ens_s,
         ens_p = evaluation_table_test_ensemble$ens_p) %>%
  select(Thresh, adaboost_s, adaboost_p, rf_s, rf_p, wsrf_s, wsrf_p, ens_s, ens_p)        

# Let's print part of the table of results.
seq <- seq(1, 451, 10)
tab <- evaluation_table_test_global[seq, ] %>% 
  mutate(Thresh = format(Thresh, digits = 4, nsmall = 3)) %>% as.data.frame()
kable(tab, "html", align = "c", digits = 0, nsmall = 0) %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 16) %>% column_spec(1:9, width = "1.5in")

rm(seq, tab, evaluation_table_test_ensemble, evaluation_table_test_global)
```

<br>

The first ensemble model culminates in a combination of 74-50, so a little bit under adaboost (74-56). Consequently, the first ensemble model will be dropped. 

<br>

#### 3. Insight Aiming at Breakthrough

<br>

Majority vote is not the unique technique to build an ensemble model. 

Having already probabilities of default from 3 selected models, it is easy to directly work on probabilities and not on outcomes predicted by the 3 models. 

<br>

### C. Second Ensemble Model: Average of Probabilities

<br>

Probabilities of default on the test set will be retrieved from the top 3 individual models, i.e. adaboost, rf and wsrf. Averages will be calculated for all observations from the test set. These averages will be the probabilities of default of the second ensemble model.

Using the probabilities of default of the second ensemble model, outcomes will be predicted for all probability thresholds and then sensitivity and precision once again will be calculated for all probability thresholds. 

Here are the results for some thresholds. 

<br>

```{r Second ensemble model (average)}

# Calculating the probabilities of the ensemble model.
prob_test_ensemble <- apply(prob_test[c("adaboost", "rf", "wsrf")], 1, mean)
prob_test_ensemble <- data.frame(prob = prob_test_ensemble)

# Storage structure for sensitivity and precision on the ensemble model 
# for all probability thresholds
evaluation_table_test_ensemble <- data.frame(Thresh = cutoffs, 
  ens_s = 1:len_cut, ens_p = 1:len_cut, stringsAsFactors = FALSE)
evaluation_table_test_ensemble <- evaluation_table_test_ensemble %>%
  mutate(Thresh = as.character(round(cutoffs, 3)))

# Predicting outcomes and calculating sensitivity and precision on the ensemble model 
# for all probability thresholds.
for (i in seq_cut) {
    refitted <- ifelse(prob_test_ensemble[, 1] > cutoffs[i], "Default", "Repaid") %>%
      factor(levels = levels(hmeq_clean_test$y))
    evaluation_table_test_ensemble[i, 2] <- 
      round(confusionMatrix(refitted, hmeq_clean_test$y)$byClass[1] * 100)
    evaluation_table_test_ensemble[i, 3] <- 
      round(confusionMatrix(refitted, hmeq_clean_test$y)$byClass[3] * 100)
    } 
rm(i, refitted)

# Table of results with the best 3 individual methods and the ensemble model 
evaluation_table_test_global <- evaluation_table_test_all %>% 
  mutate(ens_s = evaluation_table_test_ensemble$ens_s,
         ens_p = evaluation_table_test_ensemble$ens_p) %>%
  select(Thresh, adaboost_s, adaboost_p, rf_s, rf_p, wsrf_s, wsrf_p, ens_s, ens_p)

# Let's print part of the table of results.
seq <- seq(1, 451, 10)
tab <- evaluation_table_test_global[seq, ] %>%
  mutate(Thresh = format(Thresh, digits = 4, nsmall = 3)) %>% as.data.frame()

kable(tab, "html", align = "c", digits = 0, nsmall = 0) %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 16) %>% column_spec(1:9, width = "1.5in")
rm(seq, tab, evaluation_table_test_ensemble, evaluation_table_test_all)
```

<br> 

The second ensemble model comes now first before adaboost, although it is based on adaboost and 2 other models less performing than adaboost. 

In particular, 2 combinations can be noticed: 79-50 and 76-55, with surely 9 other interesting combinations in between, interesting because superior to the global target of 75-50.

Let's have a look at a graph.

<br>

```{r Graph with performance curves from second ensemble model, adaboost, rf and wsrf}
models <- c("adaboost", "rf", "wsrf", "ensemble")

graph <- evaluation_table_test_global %>% ggplot() + 
  geom_point(aes(x = adaboost_s, y = adaboost_p), color = "magenta", size = 1) +
  geom_point(aes(x = rf_s, y = rf_p), color = "green", size = 1) +
  geom_point(aes(x = wsrf_s, y = wsrf_p), color = "red", size = 1) +
  geom_point(aes(x = ens_s, y = ens_p), color = "blue", size = 1) +
  geom_rect(aes(xmin = 75, xmax = 100, ymin = 50, ymax = 100), 
            fill = "#9bc4e2", alpha = 0.01) +
  ggtitle("Performance Curves on Test Set") + 
  xlim(0, 105) + xlab("Sensitivity (recall) (%)") + 
  ylim(0, 105) + ylab("Precision (%)") +
  theme(plot.title = element_text(hjust = 0.5, size = 16, face = "bold"),
        axis.title.x = element_text(size = 16), axis.title.y = element_text(size = 16), 
        axis.text.x = element_text(size = 12), axis.text.y =element_text(size = 12))
graph

# Adding separate legend. 
tab <- data.frame(matrix(models, nrow = 1, ncol = length(models)))
names(tab) <- NULL
kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 16) %>% 
  column_spec(1, color = "magenta", background = "lightgrey", bold = TRUE) %>%
  column_spec(2, color = "green", background = "lightgrey", bold = TRUE) %>%
  column_spec(3, color = "red", background = "lightgrey", bold = TRUE) %>%
  column_spec(4, color = "blue", background = "lightgrey", bold = TRUE)

rm(graph, tab)
```

<br>

The graph above sheds some additional light: actually, the ensemble model is not better than adaboost on the whole performance curve, i.e. for all probability thresholds. On the contrary, adaboost is often better, especially on the left of the target rectangle, i.e. for the highest probability thresholds. But, in the target rectangle, the second ensemble model performs better. And that is what matters with respect to the global target. 

More particularly, the performance curve of the second ensemble model has several points on the target rectangle, which seems to offer some security when exporting the second ensemble model to other samples, actually to the validation sample hmeq_clean_val. Will that bonus be enough to meet the global target in the global validation process?

Let's have a closer look at the sequence of probability thresholds for which the ensemble model reaches the target rectangle. 

<br>

```{r Closer look at results from the second ensemble model}
seq <- seq(111, 141, 1)
tab <- evaluation_table_test_global[seq, ] %>% 
  mutate(Thresh = format(Thresh, digits = 4, nsmall = 3)) %>% as.data.frame()

kable(tab, "html", align = "c", digits = 0, nsmall = 0) %>%
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 16) %>% column_spec(1:9, width = "1.5in")
rm(seq, tab, evaluation_table_test_global)
```

<br>

The table above shows that the ensemble model hits the global target 13 times, from 0.169 with 79-50 to 0.181 with 75-54. In a snapshot, results culminate higher than with adaboost and are spread on a broader range of probability thresholds. Consequently, the second ensemble model is chosen to predict on the validation set hmeq_clean_val.

Which probability threshold is to be used? To be on the safe side, let's choose e.g. a probability threshold of 0.174 with performance metrics of 78-53. This threshold will be used when predicting on the validation set hmeq_clean_val. 

<br>

### D. Conclusion of the Analysis of the Complete Information Dataset 

<br>

The second ensemble model scores higher on the test set than the 75-50 global target. But will that performance level be maintained on the validation set? 

And let's remember that the 75-50 global target has to reached on the combination of the validation set with full information and the validation set with incomplete information. 

Consequently, analysis has to be applied to the training set hmeq_mis_train and the test set hmeq_mis_test with incomplete information.  

<br>

## VI. Modeling, Analysis and Insights about Loans with Incomplete Information

### A. Methodology

<br>

The analysis of loans with complete information has been conducted in machine learning, with an ensemble model combining the algorithms adaboost, rf and wsrf.

The analysis of loans with incomplete information will strongly combine EDA and machine learning. 

EDA has provided very valuable information about frequencies of default in case of missing values. This has been done predictor per predictor. Consequently, for each predictor, we do know the average default rate in case of missing values.  

For instance, information missing about prop_val or debt-to-inc ratio is associated with  average default rates of respectively 96 or 61 %. On the contrary, missing information about job or number of years in the current job is related to average default rates of respectively 9 or 10 %. 

We are going to make the most of it.

In datasets, missing values will be replaced with 1s and other values with zeros. In such a way, the predictors will consist in the distribution of missing values. 

Outcomes will be predicted with adaboost. Actually, other algorithms have been tried on hmeq_mis_train and hmeq_mis_test: glm, lda, naive_bayes, qda, rf, Rborist, xgbDART,  xgbTree, etc. But adaboost outperformed the other algorithms. 

<br>

### B. Results on Training Set and Test Set

<br>

Performance metrics obtained with adaboost have been summarized in the table below.

<br>

```{r Predictions on training set hmeq_mis_train and on test set hmeq_mis_test}

# Preparing hmeq_mis_train.
temp <- hmeq_mis_train[, -1]
temp <- data.frame(ifelse(is.na(temp) > 0, 1, 0))
temp <- temp %>% mutate(y = factor(hmeq_mis_train$y)) %>% select(y, everything()) %>% 
  as.data.frame()

# Predicting on the training set. Keeping "fit" for later use in validation process on validation set with incomplete information. 
set.seed(1)
fit <- train(y ~ ., method = "adaboost", data = temp)
pred <- predict(fit)
sens <- sensitivity(pred, temp$y)
prec <- posPredValue(pred, temp$y)
rm(temp, pred)

# Preparing test set.
temp2 <- hmeq_mis_test[, -1]
temp2 <- data.frame(ifelse(is.na(temp2) > 0, 1, 0))
temp2 <- temp2 %>% mutate(y = factor(hmeq_mis_test$y)) %>% select(y, everything()) %>% 
  as.data.frame()

# Predicting on test set. 
pred_test <- predict(fit, newdata = temp2)
sens_t <- sensitivity(pred_test, temp2$y)
prec_t <- posPredValue(pred_test, temp2$y)
rm(temp2, pred_test)

# Building up and printing result table. 
tab <- data.frame(matrix(nrow = 2, ncol = 2))
colnames(tab) <- c("Sensitivity", "Precision")
rownames(tab) <- c("Training Set with Incomplete Information",
                   "Test Set with Incomplete Information")
tab$Sensitivity <- c(paste(round(sens * 100), "%", sep = " "), 
                     paste(round(sens_t * 100), "%", sep = " "))
tab$Precision <- c(paste(round(prec * 100), "%", sep = " "),
                   paste(round(prec_t * 100), "%", sep = " "))

kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = c("bordered", "condensed"), 
                full_width = F, font_size = 16) %>% column_spec(1:3, width = "2in") 

rm(sens, prec, sens_t, prec_t, tab)
```

<br> 

On the training set, results are above the 75-50 global target with 94-66. But this is not very indicative of performance on the test set, as that has been seen on the training set with complete information. Indeed, there can vast overfitting. 

On the test set, there is some remarkable resilience of this model  with performance levels remaining almost at the same level as those on the training set. 

This solidly restores hopes of reaching the global target of 75-50 on the combination of the validation set with complete information and the validation set with incomplete information.

<br>

## VI. Validation Process

### A. Pre-validation Process on Loans with Complete Information

<br>

```{r Pre-validation on the validation set with complete information}

# Getting probabilities instead of outcomes from the three individual models selected: adaboost, rf and wsrf.
models <- c("adaboost", "rf", "wsrf")
fits <- fits[models]
prob_val <- lapply(fits, function(object) 
  predict(object, newdata = hmeq_clean_val, type = "prob"))
rm(fits)

# Keeping just the probabilities of default, which are for each model in the first column since "Default" is the "positive" class. 
seq <- seq(1, (length(models) * 2) - 1, 2)
prob_val <- prob_val %>% as.data.frame() %>% select(seq)
names(prob_val) <- gsub(".Default", "", names(prob_val))
rm(seq)

# Calculating the probabilities of the second ensemble model.
prob_val_ensemble <- apply(prob_val, 1, mean)
prob_val_ensemble <- data.frame(prob = prob_val_ensemble)

# Calculating predicted values with the second ensemble model for probability threshold 
# 0.174, first in numeric format for later use, then in string format.
cutoff <- 0.174
refitted_clean_val <- ifelse(prob_val_ensemble[, 1] > cutoff, 1, 0) 
buffer <- gsub(1, "Default", gsub(0, "Repaid", refitted_clean_val)) %>%
  factor(levels = levels(hmeq_clean_val$y))
rm(prob_val_ensemble)

# Creating storage structure for sensitivity and precision from the second ensemble model 
# for probability threshold 0.174.
evaluation_table_val_ensemble <- data.frame(matrix(nrow = 1, ncol = 3) * 1)
colnames(evaluation_table_val_ensemble) <- c("Threshold", "Sensitivity", "Precision")
rownames(evaluation_table_val_ensemble) <- "Validation Set with Complete Information"
evaluation_table_val_ensemble[1, 1] <- cutoff
rm(cutoff)

# Calculating sensitivity and precision.
evaluation_table_val_ensemble[1, 2] <- 
  round(confusionMatrix(buffer, hmeq_clean_val$y)$byClass[1] * 100)
evaluation_table_val_ensemble[1, 3] <- 
  round(confusionMatrix(buffer, hmeq_clean_val$y)$byClass[3] * 100)
evaluation_table_val_ensemble <- evaluation_table_val_ensemble %>% 
  mutate(Sensitivity = paste(Sensitivity, "%", sep = " ")) %>%
  mutate(Precision = paste(Precision, "%", sep = " "))
rm(buffer)

# Printing the evaluation table for the second ensemble model 
# on the validation set with complete information.
kable(evaluation_table_val_ensemble, "html", align = "c", digits = 3, nsmall = 3) %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 16) %>%         column_spec(1:3, width = "2in")
rm(evaluation_table_val_ensemble)
```

<br>

Regarding loans with complete information, the performance metrics are significantly lower on the validation set than on the test set: sensitivity is at 75 % and precision at 48 % on the validation set, against 78-53 on the test set. Consequently, on the validation set, performance metrics do not meet the 75-50 global target. So what? 

First, what about the underperformance of the second ensemble model on the validation set with complete information in comparison with the performance on the test set? Actually, it is not so surprising! Why? Because a lot of adjustments had been made on the test set in order to optimize performance metrics. Twenty-one models had been pre-tested on the test set with the standard probability threshold of 0.50! On basis of performance metrics, the best 6 had been preselected and probability thresholds had been fine-tuned. After fine-tuning, 3 had been selected. Then, an ensemble model had been built up by majority vote on outcomes from the 3 models. In the face of still insufficient performance metrics, a second ensemble model had been built up, this time based on the average of probabilities from the 3 individual models. Consequently, the second ensemble model already "knew" the test set! The test set had become an extension of the training set! The test set step was useful and valuable in the process but it was in no way any kind of validation. It was just trying a model but not checking it.

Second, what about reaching the 75-50 global target? Well, actually, the 75-50 global target applies to the whole validation set, i.e. the validation set with complete information (hmeq_clean_val), which has just been dealt with, but also the validation set with rows containing at least one missing value (hmeq_mis_val), which we are going to deal with in the next section. It is not impossible that performance metrics on the validation set with missing information overcompensate!

Consequently, let's turn to the pre-validation process on loans with missing information.

<br>

### B. Pre-validation Process on Loans with Incomplete Information

<br>

Let's apply machine learning to hmeq_mis_val, i.e. the validation set comprised of all rows containing at least one missing value. 

<br>

```{r Machine learning applied to validation set with incomplete information}

# Preparing validation set with missing information.
temp <- hmeq_mis_val[, -1]
temp <- data.frame(ifelse(is.na(temp) > 0, 1, 0))
temp <- temp %>% mutate(y = factor(hmeq_mis_val$y)) %>% select(y, everything()) %>% 
  as.data.frame()

# Predicting on test set with incomplete information. "fit" had already been obtained with train() on training set with incomplete information. 
pred_val <- predict(fit, newdata = temp)
sens <- sensitivity(pred_val, temp$y)
prec <- posPredValue(pred_val, temp$y)
rm(temp)

# Expressing "pred_val" in numerical format for later use.
refitted_mis_val <- ifelse(pred_val == "Default", 1, 0)

# Building up and printing result table. 
tab <- data.frame(matrix(nrow = 1, ncol = 2) * 1)
colnames(tab) <- c("Sensitivity", "Precision")
rownames(tab) <- c("Validation Set with Incomplete Information")
tab$Sensitivity <- paste(round(sens * 100), "%", sep = " ")
tab$Precision <- paste(round(prec * 100), "%", sep = " ")

kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 16) %>%         column_spec(1:2, width = "2in") 

rm(pred_val, sens, prec, tab)
```

<br>

On the validation set with incomplete information, the performance metrics are high: 87 % sensitivity and 66 % precision, which is much better than the 75-50 global target. 

<br>

### C. Global Validation 

<br>

Modeling and analysis have followed a double track: loans with complete information have been treated apart from loans with information missing in at least one predictor. 

But the 75-50 target applies globally, i.e. on the global validation set comprised of the validation set with complete information and the validation set with incomplete information. 

The next table renders the global performance measurement of the dual approach.

<br>

```{r Machine learning applied to global validation set}

# Getting outcomes on validation set with complete information.
y <- as.numeric(gsub("Default", 1, gsub("Repaid", 0, hmeq_clean_val$y)))  
df <- data.frame(pred = refitted_clean_val, y = y)

# Getting outcomes on validation set with incomplete information.
y <- as.numeric(gsub("Default", 1, gsub("Repaid", 0, hmeq_mis_val$y)))  
df_2 <- data.frame(pred = refitted_mis_val, y = y)
rm(y)

# Assembling both sets of outcomes.
df_3 <- rbind(df, df_2)
df_3[df_3 == 1] <- "Default"
df_3[df_3 == 0] <- "Repaid"
df_3 <- df_3 %>% mutate(pred = factor(pred), y = factor(y))
rm(df, df_2)

# Calculating sensitivity and precision.
sens <- sensitivity(df_3$pred, df_3$y)
prec <- precision(df_3$pred, df_3$y)
rm(df_3)

# Building up final result table.
tab <- data.frame(matrix(nrow = 1, ncol = 2) * 1)
colnames(tab) <- c("Sensitivity", "Precision")
rownames(tab) <- c("Global Validation Set")
tab$Sensitivity <- paste(round(sens * 100), "%", sep = " ")
tab$Precision <- paste(round(prec * 100), "%", sep = " ")
rm(sens, prec)

kable(tab, "html", align = "c") %>% 
  kable_styling(bootstrap_options = "bordered", full_width = F, font_size = 16) %>%         column_spec(1:2, width = "2in")
rm(tab)
```

<br>

## VI. Summary and Conclusion

### A. From a Data Science Point of View

<br>

The global target was 75 % sensitivity and 50 % precision. The **target has been largely exceeded with a global performance score of 84 % sensitivity and 61 % precision**. 

**Data profiling** has been pivotal by delivering insights about missing values. It has showed

- that approximately 2,500 observations out of 6,000 have been contaminated,

- that missing information is, in a vast majority of cases, related to essential information

- and that missing information is also associated with a much higher default rate, i.e. 34 % instead of 9 % on loans with complete information. 

These insights from data profiling have opened up the way to a double-track approach in modeling and analysis, loans with complete information being treated apart from loans with incomplete information.

**Exploratory data analysis** has also delivered instrumental insights: average default rate in case of missing information can be highly different according to predictor, from 96 % average default rate in case of missing information about property current market value to 9 % in case of missing values about professional occupation. This frequentist view has opened up the way to using only the "presence" of missing values as indicator of default: missing values have been replaced by 1s and other values by zeros. Then applying adaboost sufficed on its own to produce a performance score of 87 % sensitivity and 66 % precision on the validation set of loans with missing information. 

For loans with full information, **modeling** was lengthier. After pre-testing 21 algorithms, preselecting 6 of them, three algorithms have proven to outperform the numerous other ones: **adaboost, rf and wsrf** (terminology from caret).  Methodology has included a three-tier process:

- first, fine-tuning the probability threshold;

- second, building up an ensemble model by organizing a vote among outcomes from adaboost, rf and wsrf;

- third, building up a second **ensemble model** by using the mean of the probabilities generated by adaboost, rf and wsrf.

This three-tier methodology has leveraged performance scores from very poor levels in sensitivity (around 40 %) to 75 % sensitivity, with 48 % precision, on the validation set with full information. 

Globally, on the combination of the two validation sets, global performance metrics have reached 84 % sensitivity and 61 % precision, which is substantially higher than the global target of 75-50. 

<br>

### B. From a Domain Point of View

<br>

From a domain point of view, missing information should be considered a **red flag** since the default rate is 34 % on loans with missing information and only 9 % on other loans. 

More specifically, missing information in some fields should be considered a deal breaker. These fields are the property current market value (with 96 % defaults) and the debt-to-income ratio (with 61 % defaults). 

Unless, of course, the borrower has strong points that are not mentioned in the original dataset, such as e.g. capacity to pay, liquid assets or guarantors... 




